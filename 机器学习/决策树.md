# 决策树
**1、非参数学习算法  
2、可以解决多分类问题，也可以解决回归问题（将落在叶子节点取平均值）    
3、非常好的可解释性**
### 解决的核心问题：“每个节点在哪个维度做划分”和“每个维度在哪个值做划分”
### 信息熵 ： 表示随机变量不确定度的度量 。$H = -\sum_{i=1}^{k}P_{i}* log(P_{i})$
### 基尼系数 ：$G = 1 - \sum_{i=1}^{k}P_{i}^2$
### 信息熵计算比基尼系数稍慢，其他没啥区别。
## CART决策树
### 复杂度分析 ：
**一、训练 ： O(n·m·log m)  
二、预测：O(log m)**
### 剪枝：降低复杂度，解决过拟合
### 局限性：决策边界和坐标轴平行
