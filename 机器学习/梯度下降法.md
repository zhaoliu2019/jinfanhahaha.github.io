# 梯度下降法
**·不是一个机器算法，是一种基于搜索的最优化方法**  
**·作用：最优化一个损失函数**  
**ps：梯度上升法的作用是最大化一个效用函数**  
**导数可以代表方向，对应损失函数J增大的方向，我们需要得到J减小的方向来使损失函数达到最小，就可以在导数的负方向乘一个学习率n使$\theta $变化$-n\frac{dJ}{d\theta }$,最终得到的$\theta $对应的J是损失函数的最小值。**  
### ·n太小，减慢收敛速度
### ·n太大，速度变快了，但很可能导致不收敛
### ·并不是所有的函数都有唯一的极值点，解决方法：多次运行，随机化初始点，初始点也是一个超参数。

**使用梯度下降法前最好进行数据归一化（线性回归如果使用正规方程可以不用数据归一化，因为中间涉及的搜索过程很少）  
如果数值不在一个维度上，将会影响梯度的结果，而梯度乘以学习率n是我们每次真正的步长，这个步长就可能太大或太小，如果太大，导致结果不收敛，太小会导致收敛太慢，而经过归一化后能很好的解决这个问题，而且能使搜索速度大大加快！**  
**梯度下降法相较正规方程的优势，矩阵大时，正规方程会慢，而样本大的时候，批量梯度会慢可用随机梯度改进，大大增大运行效率。**  

## 随机梯度下降法
**学习率逐渐递减，只取一个固定值的话，最终很有可能已经来到最低值了但是随机的不够好，又跑出去了  
优点：  
    1、跳出局部最优解
    2、更快运行速度**
## 调试梯度下降法
**思想简单，但时间复杂度高**
## 小批量随机梯度下降法 ：中和批量梯度和随机梯度

## 再次优化我们的随机梯度下降
**问题 ：  
    将随机梯度下降法看成是下山的操作，当遇到山谷和鞍点两类地形时，会出问题，在山谷中，准确的梯度方向是沿山道向下，稍有偏离  
    就会撞向山壁，而粗糙的梯度估计使得它在两壁之间来回反弹震荡，不能沿山道方向迅速下降，导致收敛不稳定和收敛速度慢，而在鞍点初  
    ，随机梯度下降法会走入一片平坦之地（此时离最低点还很远），随机梯度下降法无法准确的察觉梯度的微小变化，结果就停滞下来。**  
**优化方法：惯性保持和环境感知  
    1、动量方法 ： 使$V_{t}$ = y·$V_{t-1}$ + n·$g_{t}$ 和 $\theta_{t+1} = \theta_{t} - V_{t}$**  
    **具体来说，前进步伐$-V_{t}$由两部分组成，一是学习速率n乘以当前估计的梯度$g_{t}$，二是带衰减的前一次步伐$V_{t-1}$  
    这里，惯性就体现在前一次步伐信息的重利用上，类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步伐好比前一时刻的速度，  
    当前步伐好比当前时刻的速度，为了计算当前时刻的速度，应当考虑前一时刻速度和当前速度共同作用的结果，因此$V_{t}$直接依赖于$V_{t-1}和g_{t}$  
    而不仅仅是$g_{t}$,另外，衰减系数y扮演了阻力的作用。**  
### 优点
**·开始训练时，积累动量，加速训练  
·局部极值附近震荡时，梯度为0，由于动量，跳出陷阱  
·梯度改变方向的时候，动量缓解动荡**  
**2、AdaGrad方法    
3、Adam方法**
    
