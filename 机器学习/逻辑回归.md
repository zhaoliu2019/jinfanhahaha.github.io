# 逻辑回归
**可以用于回归，通常被用于分类，且只能用于二分类**  
**思想：将样本的特征和样本发生的概率联系起来，这是逻辑回归的核心思想，那么我们如何将这两者联系起来呢？假设把给定每个特征一个系数，特征和系数
相乘会得到一个值，这个值便是概率，如果能能够实现这个思想，罗辑回归就基本可以实现了，首先解决一个问题，概率一般是在0～1之间，而我们要如何保证
系数乘以特征在0～1之间呢？很简单，引入Sigmoid函数，将值域映射在0～1之间，因为主要是解决分类问题，所以我们假定值域大于等于0.5，就为1，小于等于0.5 就为0，这样大体的框架就出来了，可是这个算法模型还缺少了一个灵魂，那就是惩罚，绝大多数的机器算法都带有惩罚性质，在此，我们引入损失函数，用来惩罚原来的算法模型。使损失函数达到最小值，便是最终我们需要的模型了，使用梯度下降法来求解这个损失函数，得到每个特征对应的最合适的系数。**  
**ps：这个损失函数没有正规方程解，只能用梯度下降法求,比较好的是，它是凸函数，存在全局最优解。**  

## 解题过程 ：
### Sigmoid函数 ： $\sigma \left ( t \right ) = \frac{1}{1+e^{-t}}$  值域(0,1)
### 将特征和系数代进Sigmoid函数 ： $p=\sigma \left ( \theta ^{t}X^{b} \right ) = \frac{1}{1+e^{-\theta^tX^{b} }}$ 。 y = 1 if p >= 0.5 else 0
### 引入损失函数 : cost = -y log(p) - (1-y) log(1-p)
### $J\left ( \theta  \right ) = -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(p^{(i)}) + (1-y)log(1-p^{(i)}))$
### 将损失函数和Sigmoid函数联立起来 : $J\left ( \theta  \right ) = -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(\sigma (X_{b}^{(i)}\theta )) + (1-y)log(1-\sigma\left(X_{b}^{(i)}\theta\right)))$  
### 求导后，$\bigtriangledown J(\theta ) = \frac{1}{m}X_{b}^{T}(\sigma (X_{b}\theta )-y)$

##
**由于我实现的逻辑回归只针对线性问题，如果遇到非线性问题时，使用这个逻辑回归效果往往很差，这个时候便可以用多项式回归和模型正则化的思想来优化我们的模型
，而且sklearn提供了Pipeline这么个管道方法，加上这些代码后便可以取解决非线性问题了。**
### 另外，逻辑回归如果要解决多分类问题时，可以使用OVR和OVO的方法，OVR耗时少，准确率没有OVO高，而OVO的缺点就是耗时比较高。
**sklearn也提供了接口from sklearn.multiclass import OneVsRestClassifier 和 from sklearn.multiclass import OneVsOneClassifier**
