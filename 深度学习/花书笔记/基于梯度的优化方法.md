# 基于梯度的优化方法
## Jacobian 和 Hessian 矩阵
**1、有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为 Jacobian 矩阵  
2、当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并 成一个矩阵，称为 Hessian 矩阵**  
## 抛出问题
**多维情况下，单个点处每个方向上的二阶导数是不同。Hessian 的条件数衡量 这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很
差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。梯度 下降不知道导数的这种变化，所以它不知道应该优先探索导数长期为负的方向。病 态条件也导致很难选择合适的步长。步长必须足够小，以免冲过最小而向具有较强 正曲率的方向上升。这通常意味着步长太小，以致于在其他较小曲率的方向上进展 不明显。**  
### 牛顿法解决
**我们可以使用 Hessian 矩阵的信息来指导搜索，以解决这个问题。其中最简单 的方法是 牛顿法(Newton’s method)。牛顿法基于一个二阶泰勒展开来近似 x(0) 附 近的 f(x)$\approx f(x^{0}) + f(x-x^{0})^{T}\bigtriangledown _{x}f(x^{0})^{T}H(f)(x^{0})(x-x^{0})$**  
### 接着通过计算，我们可以得到这个函数的临界点:
### $x^{*}=x^{0}-H(f)(x^{0})^{-1}\bigtriangledown _{x}f(x^{0})$
## 牛顿法特点：
**当 f 是一个正定二次函数时，牛顿法只要应用一次上式就能直接跳到函数的最 小点。如果 f 不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代上式。迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降更 快地到达临界点。这在接近局部极小点时是一个特别有用的性质，但是在鞍点附近 是有害的。当附近的临界点是最小点(Hessian 的所有特征值 都是正的)时牛顿法才适用，而梯度下降不会被吸引到鞍点(除非梯度指向鞍点)。**
