# 基于梯度的优化方法
## Jacobian 和 Hessian 矩阵
**1、有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为 Jacobian 矩阵  
2、当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并 成一个矩阵，称为 Hessian 矩阵**  
## 抛出问题
**多维情况下，单个点处每个方向上的二阶导数是不同。Hessian 的条件数衡量 这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很
差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。梯度 下降不知道导数的这种变化，所以它不知道应该优先探索导数长期为负的方向。病 态条件也导致很难选择合适的步长。步长必须足够小，以免冲过最小而向具有较强 正曲率的方向上升。这通常意味着步长太小，以致于在其他较小曲率的方向上进展 不明显。**  
### 牛顿法解决
**我们可以使用 Hessian 矩阵的信息来指导搜索，以解决这个问题。其中最简单 的方法是 牛顿法(Newton’s method)。牛顿法基于一个二阶泰勒展开来近似 x(0) 附 近的 f(x)$\approx f(x^{0}) + f(x-x^{0})^{T}\bigtriangledown _{x}f(x^{0})^{T}H(f)(x^{0})(x-x^{0})$**  
### 接着通过计算，我们可以得到这个函数的临界点:
### $x^{*}=x^{0}-H(f)(x^{0})^{-1}\triangledown _{x}f(x^{0})$
