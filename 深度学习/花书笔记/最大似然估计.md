# 最大似然估计
## 不同模型中得到特定函数作为好的 估计，而不是猜测某些函数可能是好的估计
## 步骤分解 ：
**1、考虑一组含有 m 个样本的数据集 X = {x(1), . . . , x(m)}，独立地由未知的真实数
据生成分布 $p_{data}(x)$ 生成。令 $p_{model}(x; θ)$ 是一族由 θ 确定在相同空间上的概率分布。换言之，$p_{model}(x; θ)$
将任意输入 x 映射到实数来估计真实概率 pdata(x)。对 θ 的最大似然估计被定义为:$\theta_{ML}=argmax\prod_{i=1}^{m}p_{model}(x^{i};\theta )$**  
**2、多个概率的乘积会因很多原因不便于计算。将之转化为求和形式$theta_{ML}=argmax\sum_{i=1}^{m}logp_{model}(x^{i};\theta)$**  
**3、因为当我们重新缩放代价函数时 arg max 不会改变，我们可以除以 m 得到和训练数据经验分布 $p_{data}$ 相关的期望作为准则:
$argmaxE_{x∼p_{data}}logp_{model}(x;θ)$**  
**4、一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 $p_{data}$ 和模型分布之间的差异，两者之间的差异程度可以通过 KL 散度度量。KL 散度被定义为:$D_{KL}(p_{data}||p_{model})=E_{x~p_{data}}[logp_{data}(x)-logp_{model}(x)]$。**  
**左边一项仅涉及到数据生成过程，和模型无关。这意味着当我们训练模型最小化 KL 散度时，我们只需要最小化:$-E_{x~p_{data}}[logp_{model}(x)]$**  
## 最大似然的性质
### 最大似然估计最吸引人的地方在于，它被证明当样本数目 m → ∞ 时，就收敛率而言是最好的渐近估计。
### 1、真实分布 $p_{data}$ 必须在模型族 $p_{model}$(·; θ) 中。否则，没有估计可以还原 $p_{data}$。
### 2、真实分布 $p_{data}$ 必须刚好对应一个 θ 值。否则，最大似然估计恢复出真实分布$p_{data}$ 后，也不能决定数据生成过程使用哪个 θ。

