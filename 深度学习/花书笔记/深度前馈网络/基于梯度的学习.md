# 基于梯度的学习
**我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线 性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常使
用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值;而不是像用于 训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或 SVM 的凸优化算 法那样保证全局收敛。凸优化从任何一种初始参数出发都会收敛(理论上如此—— 在实践中也很鲁棒但可能会遇到数值问题)。用于非凸损失函数的随机梯度下降没有 这种收敛性保证，并且对参数的初始值很敏感。对于前馈神经网络，将所有的权重 值初始化为小随机数是很重要的。偏置可以初始化为零或者小的正值。  
我们当然也可以用梯度下降来训练诸如线性回归和支持向量机之类的模型，并 且事实上当训练集相当大时这是很常用的。从这点来看，训练神经网络和训练其他 任何模型并没有太大区别。计算梯度对于神经网络会略微复杂一些，但仍然可以很 高效而精确地实现。**
## 代价函数
**深度神经网络设计中的一个重要方面是代价函数的选择。幸运的是，神经网络的代价函数或多或少是和其他的参数模型例如线性模型的代价函数相同的。  
在大多数情况下，我们的参数模型定义了一个分布 p(y | x; θ) 并且我们简单地 使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函 数。    有时，我们使用一个更简单的方法，不是预测 y 的完整概率分布，而是仅仅预 测在给定 x 的条件下 y 的某种统计量。某些专门的损失函数允许我们来训练这些估 计量的预测器。  用于训练神经网络的完整的代价函数，通常在我们这里描述的基本代价函数的基础上结合一个正则项。用于线性模型的权重衰减方法也直接适用于深度神经网络，而且是 最流行的正则化策略之一。**
## 使用最大似然学习条件分布
**大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价.使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计 代价函数的负担。明确一个模型 p(y | x) 则自动地确定了一个代价函数 log p(y | x)。贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的大和具 有足够的预测性，来为学习算法提供一个好的指引。饱和(变得非常平)的函数破 坏了这一目标，因为它们把梯度变得非常小。这在很多情况下都会发生，因为用于 产生隐藏单元或者输出单元的输出的激活函数会饱和。负的对数似然帮助我们在很 多模型中避免这个问题。很多输出单元都会包含一个指数函数，这在它的变量取绝 对值非常大的负值时会造成饱和。负对数似然代价函数中的对数函数消除了某些输 出单元中的指数效果。多数模型以一种特殊的形式来参数化，即它们不能表示概率零和一，但是可以无限 接近。逻辑回归是其中一个例子。对于实值的输出变量，如果模型可以控制输出分 布的密度(例如，通过学习高斯输出分布的方差参数)，那么它可能对正确的训练集 输出赋予极其高的密度，这将导致交叉熵趋向负无穷。用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就是当它 被应用于实践中经常遇到的模型时，它通常没有最小值。对于离散型输出变量，大**  
## 学习条件统计量
**有时我们并不是想学习一个完整的概率分布 p(y | x; θ)，而仅仅是想学习在给定x 时 y 的某个条件统计量。均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不 佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。这就是为什 么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在 没必要估计整个 p(y | x) 分布时。**
## 输出单元
****
