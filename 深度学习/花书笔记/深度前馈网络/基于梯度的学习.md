# 基于梯度的学习
**我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线 性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常使
用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值;而不是像用于 训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或 SVM 的凸优化算 法那样保证全局收敛。凸优化从任何一种初始参数出发都会收敛(理论上如此—— 在实践中也很鲁棒但可能会遇到数值问题)。用于非凸损失函数的随机梯度下降没有 这种收敛性保证，并且对参数的初始值很敏感。对于前馈神经网络，将所有的权重 值初始化为小随机数是很重要的。偏置可以初始化为零或者小的正值。  
我们当然也可以用梯度下降来训练诸如线性回归和支持向量机之类的模型，并 且事实上当训练集相当大时这是很常用的。从这点来看，训练神经网络和训练其他 任何模型并没有太大区别。计算梯度对于神经网络会略微复杂一些，但仍然可以很 高效而精确地实现。**
## 代价函数
**深度神经网络设计中的一个重要方面是代价函数的选择。幸运的是，神经网络的代价函数或多或少是和其他的参数模型例如线性模型的代价函数相同的。  
在大多数情况下，我们的参数模型定义了一个分布 p(y | x; θ) 并且我们简单地 使用最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为代价函 数。    有时，我们使用一个更简单的方法，不是预测 y 的完整概率分布，而是仅仅预 测在给定 x 的条件下 y 的某种统计量。某些专门的损失函数允许我们来训练这些估 计量的预测器。  用于训练神经网络的完整的代价函数，通常在我们这里描述的基本代价函数的基础上结合一个正则项。用于线性模型的权重衰减方法也直接适用于深度神经网络，而且是 最流行的正则化策略之一。**
