# 概率论
**机器学习中对概率论的大量使用**
## 不确定性的来源：
### 1、被建模系统内在的随机性
### 2、不完全观测
### 3、不完全建模
## 概率：
### 1、频率派概率：直接与事件发生的频率相联系
### 2、贝叶斯概率：涉及到确定性水平
## 离散型变量和概率质量函数（PMF）
**概率质量函数同时作用于多个随机变量，是联合概率分布**
### 若一个函数P是随机变量x的PMF，则：
**1、P 的定义域必须是 x 所有可能状态的集合  
2、∀x ∈ x, 0 ≤ P (x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。类似的，能够确保一定发生的事件概率为 1，而且不存在比这概率更 高的状态。  
3、∑P(x) = 1.我们把这条性质称之为 归一化的(normalized)。如果没有这条性质，当我们计算很多事件其中之一发生的概率时可能会得到大于 1 的概率。**  
## 连续型变量和概率密度函数（PDF）
### 如果一个函数 p 是概率密度函数，则：
**1、p 的定义域必须是 x 所有可能状态的集合。  
2、∀x ∈ x,p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1  
3、∫ p(x)dx = 1**  
## 有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为 边缘概率分布
## 条件概率
### P(y=y|x=x)=$\frac{P(y=y,x=x)}{P(x=x)}$ , P (x = x) > 0
## 链式法则 P(a,b,c)=P(a|b,c)·P(b,c)=P(a|b,c)·P(b|c)·P(c)
## 独立性，两个随机变量x，y
### 若P(x,y)=P(x)·P(y),则x，y相互独立
## 条件独立性：三个随机变量x，y，z
### 若P(x,y|z)=P(x|z)·P(y|z)，则x，y条件独立
## 期望，方差，协方差
### 期望：$E_{x~p}[f(x)] = \sum_{x}P(x)f(x)$
### 方差: $Var(f(x)) = E[(f(x)-E[f(x)])^2]$
### 标准差：方差的算术平方根
### 协方差：在某种意义上给出两个变量线性相关性的强度以及这些变量的尺度
### Cov(f(x),g(y))=E[f(x)-E[f(x)] (g(y)-E[g(y)])]
### 相关系数：将每个变量的贡献归一化，为了只衡量变量相关性而不受各个尺度大小的影响
**1、因为两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的  
2、两个变量如果协方差为零，它们之间一定没有线性关系。独立性比零协方差的要求更强，因为独立性还排除了非线性的关系。两个变量相互依赖但具有零协方差是可能的**  
### 协方差矩阵：对角元是方差
# 常用概率分布
## Bernonlli分布：单个二值随机变量的分布
### P(x=1) = a
### P(x=0) = 1-a
### P(x=x) = $a^{x}(1-a)^{1-x}$
### $E_{x}[x] = a$
### $Var_{x}(x) = a(1-a)$
## Mutinonlli分布（范畴分布）
**具有k个不同状态的单个离散型随机变量上的分布，其中k是一个有限值**
## 高斯分布（正态分布）
### $N(x;u,\sigma ^2) = \sqrt{\frac{1}{2\pi \sigma ^2}}exp(-\frac{1}{2\sigma ^2}(x-u)^2)$
**正态分布由两个参数控制，μ ∈ R 和 σ ∈ (0, ∞)。参数 μ 给出了中心峰值的坐标，这也是分布的均值:E[x] = μ。分布的标准差用 σ 表示，方差用 σ2 表示**
