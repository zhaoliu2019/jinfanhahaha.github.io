# 卷积神经网络（CNN）
### 1、数据输入层（Input Layer）
### 2、卷积计算层（CONV Layer）
### 3、激励层（ReLU Layer）
### 4、池化层（Pooling Layer）
### 5、全连接层（FC Layer）

## 数据输入层：
**去均值（只对训练集求均值）：把输入数据各个维度都化到0，CNN一般只做这个操作  
另外还有归一化，PCA，白化等等**

## 卷积计算层
### 1、局部关联，每个神经元看作一个fitter
### 2、窗口滑动，fitter对局部数据计算
**深度/depth：神经元个数  
步长/stride：滑动的长度  
填充值/zero-padding：在外围填充0，确保窗口滑到末尾是满的**
### 参数共享机制
**假设每个神经元连接数据窗口的权重是固定的**
### 固定每个神经元连接权重，可以看作模板
**1、需要估算权重个数减少，AlexNet 1亿 => 3.5w  
2、一组固定的权重和不同窗口内数据做内积：卷积**
## 激励层：把卷积输出结果做非线性映射
### 函数有：sigmoid，Tanh（双曲正切），ReLU，Leaky ReLU，ELU，Maxout
### 常用的有ReLU，收敛快，求梯度简单，但是较脆弱 f(x) = max(0,x)
### Leaky ReLu：不会饱和，计算也很快。f(x) = max(ax , x)
### ELU：所有ReLU优点都有，并且输出均值趋于0。 f(x) = x if x>0 else a(exp(x)-1)
### Maxout：计算是线性的，不会饱和，多了一些参数，由两条直线拼接而成。f(x) = max(w1·x+b1 , w2·x + b2)
### 实际经验
**1、不要用sigmoid  
2、首先用ReLU，因为快，但是要小心  
3、若ReLU不行，用Leaky ReLu或者Maxout  
4、某些情况下，Tanh效果还不错，但是情况很少**
## 池化层
### 夹在连续的卷积层中间
### 压缩数据和参数的量，减小过拟合
### Max Pooling：取出窗口中最大的值
### average Pooling：取窗口的平均值
## 全连接层 ： 尽量还原损失的信息
### 两层之间所有神经元都有权重连接
### 通常全连接层在卷积神经网络尾部
## 训练算法
### BP算法，利用链式求导法则，逐级相乘求出dw和db。
### 利用SGD，迭代更新w和b
## 优点
### 共享卷积核，对高维数据处理无压力
### 无需手动选取特征，训练好权重，即得特征分类效果好
## 缺点
### 需要调参，需要大量样本，训练最好用GPU
### 物理含义不明确
## fine-tuning
### 使用已用其他目标，预测训练好的模型的权重或者部分权重，作为初始值开始训练
### 原因：自己从头开始训练卷积神经网络容易出现问题，fine-tuning能很快的收敛到一个较理想的状态
### 做法：复用相同层的权重，新定义层取随机权重初始值，调大新定义层的学习率，调小复用层学习率
## 常用框架：Caffe ， Torch ， TensorFlow
